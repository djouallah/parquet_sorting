{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C34ja9O_XZSU"
   },
   "source": [
    "# Print Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ej6Yo5pQXuM-"
   },
   "outputs": [],
   "source": [
    "sf=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "XZag4ZSSXUqw",
    "outputId": "781967d0-b661-4905-bdb1-8b6635030483"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lineitem-SF10 ,Google Colab, 16 vCPU 59.0GB'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from psutil import *\n",
    "vCPU = str(cpu_count()) + \" vCPU\"\n",
    "mem=round(virtual_memory().total/(1024 * 1024 * 1024),0)\n",
    "runtime = 'Lineitem-SF'+str(sf)+' ,Google Colab, '+vCPU+' '+str(mem)+'GB'\n",
    "runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XwpvNhRHBWQf"
   },
   "source": [
    "# Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "zp9radlkeSGP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in /opt/conda/lib/python3.7/site-packages (11.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /opt/conda/lib/python3.7/site-packages (from pyarrow) (1.21.6)\n",
      "Collecting duckdb\n",
      "  Using cached duckdb-0.7.2.dev1623-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.9 MB)\n",
      "Installing collected packages: duckdb\n",
      "Successfully installed duckdb-0.7.2.dev1623\n",
      "Collecting polars\n",
      "  Using cached polars-0.17.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.9 MB)\n",
      "Requirement already satisfied: typing_extensions>=4.0.1 in /opt/conda/lib/python3.7/site-packages (from polars) (4.5.0)\n",
      "Installing collected packages: polars\n",
      "Successfully installed polars-0.17.0\n",
      "Collecting datafusion\n",
      "  Downloading datafusion-21.0.0.tar.gz (95 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.9/95.9 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mPreparing metadata \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[6 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Cargo, the Rust package manager, is not installed or is not on PATH.\n",
      "  \u001b[31m   \u001b[0m This package requires Rust and Cargo to compile extensions. Install it through\n",
      "  \u001b[31m   \u001b[0m the system's package manager or via https://rustup.rs/\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Checking for Rust toolchain....\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "\u001b[?25hCollecting pyspark\n",
      "  Using cached pyspark-3.3.2-py2.py3-none-any.whl\n",
      "Collecting py4j==0.10.9.5\n",
      "  Using cached py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.5 pyspark-3.3.2\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  2739    0  2739    0     0   8274      0 --:--:-- --:--:-- --:--:--  8274\n",
      "\n",
      "Will download https://builds.clickhouse.com/master/amd64/clickhouse into clickhouse.0\n",
      "\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  530M  100  530M    0     0  35.4M      0  0:00:14  0:00:14 --:--:-- 37.4M\n",
      "\n",
      "Successfully downloaded the ClickHouse binary, you can run it as:\n",
      "    ./clickhouse.0\n",
      "\n",
      "You can also install it:\n",
      "    sudo ./clickhouse.0 install\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow --upgrade\n",
    "!pip install duckdb --pre --upgrade\n",
    "!pip install polars\n",
    "!pip install datafusion=0.8\n",
    "!pip install pyspark\n",
    "!curl https://clickhouse.com/ | sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ar7WHOfAQm3"
   },
   "source": [
    "#Generate Parquet File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eVV-nZ_THdVx",
    "outputId": "4081ba0a-4321-4dd3-adb4-20bd2dc83b62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 3s, sys: 21 s, total: 5min 24s\n",
      "Wall time: 2min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import duckdb\n",
    "import pathlib\n",
    "\n",
    "\n",
    "for x in range(0, sf) :\n",
    "  con=duckdb.connect()\n",
    "  con.sql('PRAGMA disable_progress_bar;SET preserve_insertion_order=false')\n",
    "  con.sql(f\"CALL dbgen(sf={sf} , children ={sf}, step = {x})\") \n",
    "  for tbl in ['lineitem'] :\n",
    "     pathlib.Path(f'./{tbl}').mkdir(parents=True, exist_ok=True) \n",
    "     con.sql(f\"COPY (SELECT * FROM {tbl}) TO './{tbl}/{x}.parquet' \")\n",
    "  con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HeDmyVhzee8V"
   },
   "source": [
    "# DataFusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jq0cwqC50y1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 12s, sys: 19.7 s, total: 2min 32s\n",
      "Wall time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from datafusion import SessionContext\n",
    "ctx = SessionContext()\n",
    "ctx.register_parquet('lineitem', './lineitem/*.parquet')\n",
    "df = ctx.sql(\"select * from lineitem order by l_shipdate\")\n",
    "df.write_parquet(\"lineitem_Datafusion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N7ecHVITNr4F"
   },
   "source": [
    "# Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "DsXvjuwRN0U2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "JAVA_HOME is not set\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m                         \u001b[0msparkConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                     \u001b[0;31m# This SparkContext may be an existing one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m                     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m                     \u001b[0;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                     \u001b[0;31m# by all sessions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[1;32m    193\u001b[0m             )\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             self._do_init(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Java gateway process exited before sending its port number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pyspark as spark\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.master(\"local[*]\").appName(\"PySpark Read Parquet\").getOrCreate()\n",
    "spark.read.parquet('./lineitem/*.parquet').createOrReplaceTempView(\"lineitem\")\n",
    "spark.sql(\"\"\" select * from lineitem order by l_shipdate \"\"\").write.parquet(\"lineitem_spark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqhDecjmncVE"
   },
   "source": [
    "#DuckDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0HK-TCztkYmr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 49s, sys: 44.7 s, total: 4min 33s\n",
      "Wall time: 29.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import duckdb \n",
    "duckdb.sql(\n",
    "'''\n",
    "PRAGMA disable_progress_bar;\n",
    "COPY  (select * from './lineitem/*.parquet' order by l_shipdate )\n",
    "to 'lineitem_DuckDB' (FORMAT 'PARQUET', PER_THREAD_OUTPUT TRUE)\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JziMdMTGBpLD"
   },
   "source": [
    "# Clickhouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "iQFJpOOz_D2S"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.61 s, sys: 520 ms, total: 2.13 s\n",
      "Wall time: 1min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Query 1\n",
    "!./clickhouse local --query=\" \\\n",
    "    select * FROM file ('./lineitem/*.parquet',Parquet) \\\n",
    "    order by l_shipdate \\\n",
    "    INTO OUTFILE 'clickhouse.parquet' \\\n",
    "    FORMAT Parquet \\\n",
    "    \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-odTmS2dikyT"
   },
   "source": [
    "# Polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "GlEMX0jOcJ4M"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STREAMING CHUNK SIZE: 3125 rows\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 29s, sys: 18.6 s, total: 2min 48s\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import polars as pl\n",
    "pl.scan_parquet(\"./lineitem/*.parquet\").sort(\"l_shipdate\").sink_parquet(\"lineitem_sort_polars.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IpjjaxGw1s5b"
   },
   "source": [
    "# Pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "4zgt3Q--10mM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 27s, sys: 12 s, total: 2min 39s\n",
      "Wall time: 2min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pyarrow.dataset as ds\n",
    "lineitem = ds.dataset('./lineitem',format=\"parquet\").sort_by('l_shipdate')\n",
    "ds.write_dataset(lineitem,'./pyarrow', format=\"parquet\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "local-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
